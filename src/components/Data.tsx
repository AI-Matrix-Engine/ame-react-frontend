import { PersonIcon, ChatBubbleIcon } from "@radix-ui/react-icons";
export const menuItems = [
  {
    category: "App Name Here",
    items: [
      {
        itemCategory: "AI Prolog",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
      {
        itemCategory: "Carry",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
    ],
  },
  {
    category: "Intelligence Engine - From Components",

    items: [
      {
        itemCategory: "AI API",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Create", "Run", "Manage", "Admin", "Login", "Signup"],
      },
      {
        itemCategory: "Chatbot",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Open AI",
          "Google Gemini Pro",
          "AI infinity Matrix",
          "Chatbot Template",
          "Admin",
          "Login",
        ],
      },
      {
        itemCategory: "Workflows",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Start Process(live)",
          "Build",
          "Builder 3",
          "Drag & Drop 1",
          "Tables",
        ],
      },
      {
        itemCategory: "Other Intellegence",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Sub_Item 1", "Sub _Item 2", "Sub_Item 3"],
      },
    ],
  },
  {
    category: "Admin Development",

    items: [
      {
        itemCategory: "OAI App Dev",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Sample Form Element",
          "Sample Tab Content",
          "Run",
          "Manage",
          "Admin",
        ],
      },
    ],
  },
  {
    category: "Static Menu",

    items: [
      {
        itemCategory: "Search Console",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Accordians", "BreadCrumbs", "Cards"],
      },
      {
        itemCategory: "Buttons",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Google Search",
          "ButtonGroups",
          "LoadingButton",
          "Dropdowns",
        ],
      },
      {
        itemCategory: "Keywords Search",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Form Control",
          "Select",
          "MultiSelect",
          "Check & radios",
          "Range",
          "Input group",
          "Floating Labels",
          "Date Picker",
          "Date Range Picker",
          "Time Picker",
          "Layout",
          "Validation",
        ],
      },
      {
        itemCategory: "Content Generator",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Core UI Icons",
          "Core UI Icons Brands",
          "Core UI Icons Flag",
        ],
      },
      {
        itemCategory: "Shopify",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Alert", "Badge", "Modals", "Toasts"],
      },
      {
        itemCategory: "WordPress",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
    ],
  },
  {
    category: "Knowledge",

    items: [
      {
        itemCategory: "Webscrapers",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
      {
        itemCategory: "Crawler",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
      {
        itemCategory: "Website Analyzer",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
      {
        itemCategory: "Google Maps",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
    ],
  },
  {
    category: "Extras",

    items: [
      {
        itemCategory: "Pages",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Login", "Register", "Error 404", "Error 500"],
      },
      {
        itemCategory: "Apps",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: [
          "Invoicing",
          "Email",
          //  {subMenuItem:"Invoicing",subMenuItemOptions:["Invoic"]},
          //  {subMenuItem:"Email",subMenuItemOptions:["Inbox","Message","Compose"]}
        ],
      },

      {
        itemCategory: "Actual UI Doc",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
      },
    ],
  },
  {
    category: "Section Title",

    items: [
      {
        itemCategory: "Parent Menu 1",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Sub-item 1", "Sub-item 2", "Sub-item 3"],
      },
      {
        itemCategory: "Parent Menu 2",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Sub-item 1", "Sub-item 2", "Sub-item 3"],
      },
      {
        itemCategory: "Parent Menu 3",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Sub-item 1", "Sub-item 2", "Sub-item 3"],
      },
      {
        itemCategory: "Parent Menu 4",
        icon: <PersonIcon className="h-4 w-4 text-white" />,
        itemSubMenu: ["Sub-item 1", "Sub-item 2", "Sub-item 3"],
      },
    ],
  },
];

export const promptData = [
  {
    role: "system",
    text: ``,
  },
  {
    role: "user",
    text: `I’m going on one of the most amazing trips of life and need you to help me decide on the best places to visit. I’m going to be visiting New York, Colorado and areas around San Francisco and it’s really important that I go to the best spots! Some of my favorite things to do are: Biking, walking, going to dinner and enjoying the city! This is really important for me so please give me the best places I should go!`,
  },
];

export const responseData = [
  {
    _id: "6616e8d7c4dd135b3e82fddb",
    model: "gpt-4-0125-preview",
    name: "GPT 4 0125 Preview",
    class: "gpt-4",
    type: "text_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_chat",
    },
    limitations: {
      context_window: 128000,
      max_tokens: 4095,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4095,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fddc",
    model: "gpt-4-1106-vision-preview",
    name: "GPT 4 Vision",
    class: "gpt-4v",
    type: "text_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_chat",
    },
    limitations: {
      context_window: 4096,
      max_tokens: 4096,
      capabilities: ["text", "image"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4096,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fddd",
    model: "gpt-3.5-turbo",
    name: "GPT 3.5 Turbo",
    class: "gpt-3",
    type: "text_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_chat",
    },
    limitations: {
      context_window: 16385,
      max_tokens: 4096,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4096,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fdde",
    model: "ft:gpt-3.5-turbo-0613:titanium::8bxZrvqi",
    name: "Matrix LSI Fine-Tuned",
    class: "gpt-3-ft",
    type: "text_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_chat",
    },
    limitations: {
      context_window: 4096,
      max_tokens: 4096,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4096,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fddf",
    model: "ft:gpt-3.5-turbo-0613:titanium:matrix-ama:996BDtM4",
    name: "Matrix AMA Fine-Tuned",
    class: "gpt-3-ft",
    type: "text_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_chat",
    },
    limitations: {
      context_window: 4096,
      max_tokens: 4096,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4096,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde0",
    model: "dall-e-3",
    name: "DALL-E 3",
    class: "dall-e",
    type: "image_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_dalle",
    },
    limitations: {
      context_window: 4096,
      max_tokens: 4096,
      capabilities: ["image"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4096,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde1",
    model: "text-embedding-3-large",
    name: "Text Embedding 3 Large",
    class: "oai-embed",
    type: "embeddings",
    api: {
      provider: "OpenAI",
      endpoint: "oai_embedding",
    },
    limitations: {
      context_window: null,
      max_tokens: 0,
      capabilities: ["text_embedding"],
    },
    controls: [
      {
        id: "encoding_format",
        componentType: "dropdown",
        label: "Encoding format",
        helpText: "",
        type: "string",
        values: ["float", "base64"],
      },
      {
        id: "dimensions",
        componentType: "input",
        label: "Dimensions",
        helpText: "",
        type: "int",
        value: "",
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde2",
    model: "claude-3-opus-20240229",
    name: "Claud 3 Opus",
    class: "claude-high",
    type: "text_generation",
    api: {
      provider: "Anthropic",
      endpoint: "claude",
    },
    limitations: {
      context_window: 200000,
      max_tokens: 4000,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4000,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde3",
    model: "claude-3-sonnet-20240229",
    name: "Claud 3 Sonnet",
    class: "claude-mid",
    type: "text_generation",
    api: {
      provider: "Anthropic",
      endpoint: "claude",
    },
    limitations: {
      context_window: 200000,
      max_tokens: 4000,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4000,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde4",
    model: "claude-3-haiku-20240307",
    name: "Claud 3 Haiku",
    class: "claude-low",
    type: "text_generation",
    api: {
      provider: "Anthropic",
      endpoint: "claude",
    },
    limitations: {
      context_window: 200000,
      max_tokens: 4000,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 4000,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde5",
    model: "gemini-1.0-pro",
    name: "Google Gemini Pro 1.0",
    class: "gemini-pro",
    type: "text_generation",
    api: {
      provider: "Google",
      endpoint: "vertexai",
    },
    limitations: {
      context_window: 32000,
      max_tokens: 8192,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: null,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 0.95,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "top_k",
        componentType: "slider",
        label: "Top K",
        helpText:
          "The number of most likely tokens to keep. 0 means no truncation.",
        type: "int",
        value: 0,
        min: 0,
        max: 40,
        step: 1,
      },
      {
        id: "max_responses",
        componentType: "slider",
        label: "Max Responses",
        helpText: "The maximum number of responses to return.",
        type: "int",
        value: 1,
        min: 1,
        max: 8,
        step: 1,
      },
    ],
  },
  {
    _id: "6616e8d7c4dd135b3e82fde6",
    model: "gemini-1.5-pro",
    name: "Google Gemini Pro 1.5",
    class: "gemini-pro",
    type: "text_generation",
    api: {
      provider: "Google",
      endpoint: "vertexai",
    },
    limitations: {
      context_window: 1000000,
      max_tokens: 8192,
      capabilities: ["text", "image"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: null,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 0.95,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "top_k",
        componentType: "slider",
        label: "Top K",
        helpText:
          "The number of most likely tokens to keep. 0 means no truncation.",
        type: "int",
        value: 0,
        min: 0,
        max: 40,
        step: 1,
      },
      {
        id: "max_responses",
        componentType: "slider",
        label: "Max Responses",
        helpText: "The maximum number of responses to return.",
        type: "int",
        value: 1,
        min: 1,
        max: 8,
        step: 1,
      },
    ],
  },
  {
    _id: "66247a3ab419895d0549ff91",
    model: "llama3-70b-8192",
    name: "LLaMa 3 70b 8k",
    class: "llama3-70b",
    type: "text_generation",
    api: {
      provider: "Groq Meta",
      endpoint: "groq",
    },
    limitations: {
      context_window: 8192,
      max_tokens: 2048,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 2048,
        step: 1,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
    ],
  },
  {
    _id: "66247b368d760a1f8eac4446",
    model: "llama3-8b-8192",
    name: "LLaMa 3 8b 8k",
    class: "llama3-8b",
    type: "text_generation",
    api: {
      provider: "Groq Meta",
      endpoint: "groq",
    },
    limitations: {
      context_window: 8192,
      max_tokens: 2048,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 2048,
        step: 1,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
    ],
  },
  {
    _id: "66247b6bcc7a4ccd7c4db47c",
    provider: "Groq Meta",
    model: "llama2-70b-4096",
    class: "groq-llama-big-low",
    name: "LLaMa 2 70b 4k",
    api: "groq",
    context_window: 4096,
    max_tokens: 4096,
    type: "text_generation",
    capabilities: ["text", "unknown"],
  },
  {
    _id: "66247bb1c52345456d90db63",
    model: "mixtral-8x7b-32768",
    name: "Mistral 8x 70b 32k",
    class: "llama3-70b",
    type: "text_generation",
    api: {
      provider: "Groq Mistral",
      endpoint: "groq",
    },
    limitations: {
      context_window: 32768,
      max_tokens: 2048,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: 2048,
        step: 1,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
    ],
  },
  {
    _id: "66247bfb5dcb804acb4803c5",
    provider: "Groq Google",
    model: "gemma-7b-it",
    class: "gemma-7b",
    name: "Gemma 7b",
    api: "groq",
    context_window: 8192,
    max_tokens: 8192,
    type: "text_generation",
    capabilities: ["text", "unknown"],
  },
  {
    _id: "6625430f1d3ee1036b1db4c6",
    model: "gpt-4-turbo-2024-04-09",
    name: "GPT 4 Turbo 2024-04-09",
    class: "gpt-4",
    type: "text_generation",
    api: {
      provider: "OpenAI",
      endpoint: "oai_chat",
    },
    limitations: {
      context_window: 128000,
      max_tokens: 4095,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: null,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "stop_sequence",
        componentType: "input",
        label: "Stop Sequence",
        helpText:
          "The model will stop generating tokens when this sequence is generated.",
        type: "string",
        value: "",
      },
      {
        id: "frequency_penalty",
        componentType: "slider",
        label: "Frequency Penalty",
        helpText:
          "The higher the frequency penalty, the more likely the model is to repeat itself.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "presence_penalty",
        componentType: "slider",
        label: "Presence Penalty",
        helpText:
          "How much to penalize new tokens based on whethere they appear in the text so far. Increases the model's likelihood to talk about new topics.",
        type: "float",
        value: 0.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "json_object",
        componentType: "switch",
        label: "JSON Object",
        helpText: "Return the response as a JSON object.",
        type: "boolean",
        value: false,
      },
      {
        id: "seed",
        componentType: "input",
        label: "Seed",
        helpText:
          "Seed used for sampling. The API attempts to return the same response to the same request with an identical seed. If empty, a random seed is used.",
        type: "int",
        value: "",
      },
      {
        id: "tools",
        componentType: "switchGroup",
        label: "Tools",
        helpText: "Enable or disable tools.",
        choices: [
          {
            id: "search",
            label: "Search",
            value: false,
          },
          {
            id: "files",
            label: "Files",
            value: false,
          },
          {
            id: "code_interpreter",
            label: "Code Interpreter",
            value: false,
          },
          {
            id: "functions",
            label: "Functions",
            value: false,
          },
        ],
      },
    ],
  },
  {
    _id: "6625430f1d3ee1036b1db617",
    model: "gemini-1.5-pro-preview-0409",
    name: "Google Gemini Pro 1.5 Vision",
    class: "gemini-vision",
    type: "image_generation",
    api: {
      provider: "Google",
      endpoint: "vertexai",
    },
    limitations: {
      context_window: null,
      max_tokens: null,
      capabilities: ["image"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: null,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 0.95,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "top_k",
        componentType: "slider",
        label: "Top K",
        helpText:
          "The number of most likely tokens to keep. 0 means no truncation.",
        type: "int",
        value: 0,
        min: 0,
        max: 40,
        step: 1,
      },
      {
        id: "max_responses",
        componentType: "slider",
        label: "Max Responses",
        helpText: "The maximum number of responses to return.",
        type: "int",
        value: 1,
        min: 1,
        max: 8,
        step: 1,
      },
    ],
  },
  {
    _id: "6625430f1d3ee1036b1db699",
    model: "gpt-3.5-turbo-0125",
    api: "oai_chat",
    capabilities: ["text"],
    class: "gpt-3",
    context_window: 16000,
    max_tokens: 4095,
    name: "",
    provider: "OpenAI",
    type: "chat_completions",
  },
  {
    _id: "662543101d3ee1036b1db7d8",
    model: "specific-model-name-1234",
    api: "provider_api",
    capabilities: [
      "text",
      "image",
      "video",
      "audio",
      "search",
      "other",
      "unknown",
    ],
    class: "model-class",
    context_window: 200000,
    max_tokens: 4096,
    name: "Common Display Name",
    provider: "Some Company Name",
    type: "primary-purpose",
  },
  {
    _id: "662aca302d62bb8e16126dce",
    model: "gemini-1.0-pro-002",
    name: "Google Gemini Pro 1.0",
    class: "gemini-pro",
    type: "text_generation",
    api: {
      provider: "Google",
      endpoint: "vertexai",
    },
    limitations: {
      context_window: 32000,
      max_tokens: 8192,
      capabilities: ["text"],
    },
    controls: [
      {
        id: "temperature",
        componentType: "slider",
        label: "Temperature",
        helpText:
          "Controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        type: "float",
        value: 1.0,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      },
      {
        id: "max_tokens",
        componentType: "slider",
        label: "Max Tokens",
        helpText: "The maximum number of tokens to generate.",
        type: "int",
        value: 60,
        min: 1,
        max: null,
        step: 1,
      },
      {
        id: "top_p",
        componentType: "slider",
        label: "Top P",
        helpText:
          "The cumulative probability of the most likely tokens to sample from. 1.0 means no truncation.",
        type: "float",
        value: 0.95,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      },
      {
        id: "top_k",
        componentType: "slider",
        label: "Top K",
        helpText:
          "The number of most likely tokens to keep. 0 means no truncation.",
        type: "int",
        value: 0,
        min: 0,
        max: 40,
        step: 1,
      },
      {
        id: "max_responses",
        componentType: "slider",
        label: "Max Responses",
        helpText: "The maximum number of responses to return.",
        type: "int",
        value: 1,
        min: 1,
        max: 8,
        step: 1,
      },
    ],
  },
];